{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0d712c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and setup paths\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Paths to the data\n",
    "data_dir = os.path.join(root_dir, \"datasets\")\n",
    "models_dir = os.path.join(root_dir, \"models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a47ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature map length: 7\n",
      "Calculated flattened dimension: 896\n"
     ]
    }
   ],
   "source": [
    "#Configure training parameters\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "# Get input and target data paths\n",
    "input_data = f\"{data_dir}/RDFs/nonMin_nCu_38.pt\"\n",
    "target_data = f\"{data_dir}/RDFs/min_nCu_38.pt\"\n",
    "\n",
    "# Enable DDP\n",
    "use_ddp = True\n",
    "use_data_parallel = False\n",
    "\n",
    "# Model architecture parameters\n",
    "embed_dim = 3\n",
    "in_channels = 1\n",
    "input_length = 340\n",
    "n_features = 340 * in_channels\n",
    "\n",
    "# Encoder architecture\n",
    "channels = [8, 16, 32, 64, 64, 128]  # out_channels for each Conv1d\n",
    "kernel_sizes = [3, 3, 3, 3, 3, 3]\n",
    "paddings = [2, 2, 2, 2, 2, 2]\n",
    "strides = [1, 1, 1, 1, 1, 1]\n",
    "pool_kernel = 2\n",
    "pool_stride = 2\n",
    "\n",
    "length = input_length\n",
    "for i in range(len(channels)):\n",
    "    length = math.floor((length + 2*paddings[i] - (kernel_sizes[i]-1) - 1)/strides[i] + 1)\n",
    "    length = math.floor((length - pool_kernel)/pool_stride + 1)\n",
    "\n",
    "last_layer_length = channels[-1]\n",
    "flat_dim = last_layer_length * length\n",
    "feature_map_length = length\n",
    "print(f\"Final feature map length: {length}\")\n",
    "print(f\"Calculated flattened dimension: {flat_dim}\")\n",
    "\n",
    "params = {\n",
    "  \"GENERAL\": {\n",
    "    \"device\": \"cuda\",\n",
    "    \"seed\": 42,\n",
    "    \"apply_jit_tracing\": False,\n",
    "    \"use_data_parallel\": use_data_parallel,\n",
    "    \"use_ddp\": use_ddp,\n",
    "    \"use_lr_finder\": False,\n",
    "    \"use_tensorboard\": False,\n",
    "    \"input_length\": 340,\n",
    "    \"input_channels\": in_channels,\n",
    "    \"input_features\": n_features,\n",
    "    \"flattened_features\": n_features\n",
    "  },\n",
    "  \"MODEL\": {\n",
    "    \"type\": \"autoencoder\",\n",
    "    \"input_shape\": [in_channels, n_features],\n",
    "    \"embedding_dim\": embed_dim,\n",
    "    \"flattened_dim\": flat_dim,\n",
    "    \"feature_map_length\": feature_map_length,\n",
    "    \"encoder_layers\": [\n",
    "      [\n",
    "        {\"type\": \"Conv1d\", \"in_channels\": in_channels, \"out_channels\": 8, \"kernel_size\": 3, \"padding\": 2},\n",
    "        {\"type\": \"MaxPool1d\", \"kernel_size\": 2, \"stride\": 2},\n",
    "        {\"type\": \"ReLU\"},\n",
    "        {\"type\": \"BatchNorm1d\", \"num_features\": 8}\n",
    "      ],\n",
    "      [\n",
    "        {\"type\": \"Conv1d\", \"in_channels\": 8, \"out_channels\": 16, \"kernel_size\": 3, \"padding\": 2},\n",
    "        {\"type\": \"MaxPool1d\", \"kernel_size\": 2, \"stride\": 2},\n",
    "        {\"type\": \"ReLU\"},\n",
    "        {\"type\": \"BatchNorm1d\", \"num_features\": 16}\n",
    "      ],\n",
    "      [\n",
    "        {\"type\": \"Conv1d\", \"in_channels\": 16, \"out_channels\": 32, \"kernel_size\": 3, \"padding\": 2},\n",
    "        {\"type\": \"MaxPool1d\", \"kernel_size\": 2, \"stride\": 2},\n",
    "        {\"type\": \"ReLU\"},\n",
    "        {\"type\": \"BatchNorm1d\", \"num_features\": 32}\n",
    "      ],\n",
    "      [\n",
    "        {\"type\": \"Conv1d\", \"in_channels\": 32, \"out_channels\": 64, \"kernel_size\": 3, \"padding\": 2},\n",
    "        {\"type\": \"MaxPool1d\", \"kernel_size\": 2, \"stride\": 2},\n",
    "        {\"type\": \"ReLU\"},\n",
    "        {\"type\": \"BatchNorm1d\", \"num_features\": 64}  \n",
    "      ],\n",
    "      [\n",
    "        {\"type\": \"Conv1d\", \"in_channels\": 64, \"out_channels\": 64, \"kernel_size\": 3, \"padding\": 2},\n",
    "        {\"type\": \"MaxPool1d\", \"kernel_size\": 2, \"stride\": 2},\n",
    "        {\"type\": \"ReLU\"},\n",
    "        {\"type\": \"BatchNorm1d\", \"num_features\": 64}\n",
    "      ],\n",
    "      [\n",
    "        {\"type\": \"Conv1d\", \"in_channels\": 64, \"out_channels\": 128, \"kernel_size\": 3, \"padding\": 2},\n",
    "        {\"type\": \"MaxPool1d\", \"kernel_size\": 2, \"stride\": 2},\n",
    "        {\"type\": \"ReLU\"},\n",
    "        {\"type\": \"BatchNorm1d\", \"num_features\": 128}\n",
    "      ]\n",
    "    ],\n",
    "    \"bottleneck_layers\": [\n",
    "      [\n",
    "        {\"type\": \"Flatten\"},\n",
    "        {\"type\": \"Linear\", \"in_features\": flat_dim, \"out_features\": embed_dim},\n",
    "      ],\n",
    "      [\n",
    "        {\"type\": \"Linear\", \"in_features\": embed_dim, \"out_features\": flat_dim},\n",
    "        {\"type\": \"Sigmoid\"}\n",
    "      ]\n",
    "    ],\n",
    "    \"decoder_layers\": [\n",
    "      [\n",
    "        {\"type\": \"Upsample\", \"scale_factor\": 2},\n",
    "        {\"type\": \"ConvTranspose1d\", \"in_channels\": last_layer_length, \"out_channels\": 64, \"kernel_size\": 3, \"padding\": 2},\n",
    "        {\"type\": \"ReLU\"},\n",
    "        {\"type\": \"BatchNorm1d\", \"num_features\": 64}\n",
    "      ],\n",
    "      [\n",
    "        {\"type\": \"Upsample\", \"scale_factor\": 2},\n",
    "        {\"type\": \"ConvTranspose1d\", \"in_channels\": 64, \"out_channels\": 64, \"kernel_size\": 3, \"padding\": 2},\n",
    "        {\"type\": \"ReLU\"},\n",
    "        {\"type\": \"BatchNorm1d\", \"num_features\": 64}\n",
    "      ],\n",
    "      [\n",
    "        {\"type\": \"Upsample\", \"scale_factor\": 2},\n",
    "        {\"type\": \"ConvTranspose1d\", \"in_channels\": 64, \"out_channels\": 32, \"kernel_size\": 3, \"padding\": 2},\n",
    "        {\"type\": \"ReLU\"},\n",
    "        {\"type\": \"BatchNorm1d\", \"num_features\": 32}\n",
    "      ],\n",
    "      [\n",
    "        {\"type\": \"Upsample\", \"scale_factor\": 2},\n",
    "        {\"type\": \"ConvTranspose1d\", \"in_channels\": 32, \"out_channels\": 16, \"kernel_size\": 3, \"padding\": 2},\n",
    "        {\"type\": \"ReLU\"},\n",
    "        {\"type\": \"BatchNorm1d\", \"num_features\": 16}\n",
    "      ],\n",
    "      [\n",
    "        {\"type\": \"Upsample\", \"scale_factor\": 2},\n",
    "        {\"type\": \"ConvTranspose1d\", \"in_channels\": 16, \"out_channels\": 8, \"kernel_size\": 3, \"padding\": 2},\n",
    "        {\"type\": \"ReLU\"},\n",
    "        {\"type\": \"BatchNorm1d\", \"num_features\": 8}\n",
    "      ],\n",
    "      [\n",
    "        {\"type\": \"Upsample\", \"scale_factor\": 2},\n",
    "        {\"type\": \"Conv1d\", \"in_channels\": 8, \"out_channels\": in_channels, \"kernel_size\": 3, \"padding\": 2},\n",
    "      ]\n",
    "    ]\n",
    "  },\n",
    "  \"TRAINING\": {\n",
    "    # Training parameters\n",
    "    \"batch_size\": 1536,\n",
    "    \"train_size\": 0.8,\n",
    "    \"min_epochs\": 150,\n",
    "    \"max_epochs\": 500,\n",
    "    \"loss_function\": \"MSELoss\",\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"normalization\": \"minmax\",\n",
    "    # Optimizer parameters\n",
    "    \"num_workers\": 16,\n",
    "    \"pin_memory\": True,\n",
    "    \"scheduled_lr\": False,\n",
    "    \"scheduler_params\": {\n",
    "      \"lr_warmup_epochs\": 50,\n",
    "      \"milestones\": [],\n",
    "        \"gamma\": 0.5\n",
    "    },\n",
    "    \"early_stopping\": True,\n",
    "    \"early_stopping_params\": {\n",
    "      \"patience\": 30,\n",
    "      \"min_delta\": 0.0001\n",
    "    },\n",
    "  },\n",
    "  \"INPUTS\": {\n",
    "    \"dataset\": input_data,\n",
    "    \"target\": target_data\n",
    "  }\n",
    "}\n",
    "\n",
    "# Save json configuration\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "#run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_id = \"nonMin_to_min_nCu_38\"\n",
    "model_id_dir = f\"{models_dir}/{run_id}\"\n",
    "os.makedirs(model_id_dir, exist_ok=True)\n",
    "\n",
    "with open(f\"{model_id_dir}/config.json\", 'w') as f:\n",
    "    json.dump(params, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b23f998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Configuration Summary ===\n",
      "Run ID: nonMin_to_min_nCu_38\n",
      "DDP Enabled: True\n",
      "Batch Size: 1536\n",
      "Number of Workers: 16\n",
      "Learning Rate: 0.001\n",
      "\n",
      "=== Available CPU Resources ===\n",
      "CPUs available to PyTorch: 48\n",
      "Num of OpenMP threads: 16\n",
      "\n",
      "=== Available GPU Resources ===\n",
      "GPUs available to PyTorch: 3\n",
      "Available GPU devices: ['NVIDIA A30', 'NVIDIA A30', 'NVIDIA A30']\n"
     ]
    }
   ],
   "source": [
    "# Set up environment variables\n",
    "import random\n",
    "master_port = random.randint(29500, 30000)  # Random port for DDP\n",
    "os.environ.update({\n",
    "    #\"NCCL_DEBUG\": \"INFO\",                          # Enable NCCL debugging\n",
    "    \"NCCL_SOCKET_IFNAME\": \"^lo,docker\",             # Skip loopback and docker interfaces\n",
    "    \"NCCL_IB_DISABLE\": \"0\",                         # Enable InfiniBand if available\n",
    "    \"NCCL_P2P_DISABLE\": \"0\",                        # Ensure P2P is enabled\n",
    "    \"TORCH_NCCL_BLOCKING_WAIT\": \"1\",                # Use blocking wait for better performance\n",
    "    \"NCCL_LL_THRESHOLD\": \"0\",                       # Disable low latency threshold \n",
    "    \"MASTER_PORT\": str(master_port),                # Random port for DDP\n",
    "    \"MASTER_ADDR\": \"localhost\",                     # Master address for DDP\n",
    "    \"WORLD_SIZE\": str(torch.cuda.device_count()),   # Total number of possible processes\n",
    "    \"OMP_NUM_THREADS\": \"16\",                        # Set OpenMP threads to 16\n",
    "    \"MKL_THREADING_LAYER\": \"INTEL\",                 # Set MKL threading layer to Intel\n",
    "    \"KMP_BLOCKTIME\": \"0\",                           # Set KMP block time to 0\n",
    "    \"KMP_AFFINITY\": \"granularity=fine,compact,1,0\", # Set KMP affinity\n",
    "    \"KMP_HW_SUBSET\": \"1t\",                          # Use only physical cores, no hyperthreading\n",
    "    \"I_MPI_PIN_DOMAIN\": \"auto\",                     # Automatically pin MPI processes to cores\n",
    "    \"I_MPI_PIN\": \"ON\",                              # Enable process pinning\n",
    "    \"I_MPI_PIN_CELL\": \"core\",                       # Pin MPI processes to cores\n",
    "    \"CUDA_VISIBLE_DEVICES\": \",\".join(str(i) for i in range(torch.cuda.device_count())),\n",
    "    \"PYTHONPATH\": f\"{root_dir}:{os.environ.get('PYTHONPATH', '')}\"\n",
    "})\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"\\n=== Configuration Summary ===\")\n",
    "print(f\"Run ID: {run_id}\")\n",
    "print(f\"DDP Enabled: {use_ddp}\")\n",
    "print(f\"Batch Size: {params['TRAINING']['batch_size']}\")\n",
    "print(f\"Number of Workers: {params['TRAINING']['num_workers']}\")\n",
    "print(f\"Learning Rate: {params['TRAINING']['learning_rate']}\")\n",
    "\n",
    "# Check for generic variables that might be available\n",
    "torch.set_num_threads(int(os.environ.get('OMP_NUM_THREADS', 1)))\n",
    "print(\"\\n=== Available CPU Resources ===\")\n",
    "print(f\"CPUs available to PyTorch: {torch.get_num_interop_threads()}\")\n",
    "print(f\"Num of OpenMP threads: {torch.get_num_threads()}\")\n",
    "\n",
    "print(\"\\n=== Available GPU Resources ===\")\n",
    "print(f\"GPUs available to PyTorch: {torch.cuda.device_count()}\")\n",
    "print(f\"Available GPU devices: {[torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1ed6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: torchrun --nproc_per_node=3 --nnodes=1 /scratch/rasera/PyISV/PyISV/scripts/train_autoencoder.py --config /scratch/rasera/PyISV/models//nonMin_to_min_nCu_38/config.json --models_dir /scratch/rasera/PyISV/models/ --run_id nonMin_to_min_nCu_38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDP] Detected torchrun: LOCAL_RANK=0, RANK=0, WORLD_SIZE=3\n",
      "[DDP] Detected torchrun: LOCAL_RANK=2, RANK=2, WORLD_SIZE=3\n",
      "ℹ️ Using GPU backend: nccl\n",
      "[DDP] Detected torchrun: LOCAL_RANK=1, RANK=1, WORLD_SIZE=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank2]:[W525 13:40:56.080434973 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "[rank1]:[W525 13:40:56.080434952 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "[rank0]:[W525 13:40:56.282142841 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DDP initialized successfully with 3 processes using nccl backend\n",
      "\n",
      "Training on cuda with run ID: nonMin_to_min_nCu_38\n",
      "\n",
      "ℹ️ Model type: <class 'torch.nn.parallel.distributed.DistributedDataParallel'>\n",
      "ℹ️ Device: cuda:0\n",
      "ℹ️ Use DDP: True\n",
      "ℹ️ Train loader length: 12\n",
      "ℹ️ Loss function: MSELoss()\n",
      "\n",
      "▶️ Starting training from epoch 0 to 500\n",
      "\n",
      "⏳ [Epoch 10] - train loss: 0.0076 - validation loss: 0.0075 - (1.18s/epoch)\n",
      "⏳ [Epoch 10] - train loss: 0.0076 - validation loss: 0.0076 - (1.18s/epoch)\n",
      " --- 💾 Saving best model at epoch: 10 --- \n",
      "⏳ [Epoch 10] - train loss: 0.0077 - validation loss: 0.0074 - (1.18s/epoch)\n",
      " --- 💾 Saving best model at epoch: 20 --- \n",
      "⏳ [Epoch 20] - train loss: 0.0065 - validation loss: 0.0066 - (0.83s/epoch)\n",
      "⏳ [Epoch 20] - train loss: 0.0064 - validation loss: 0.0063 - (0.83s/epoch)\n",
      "⏳ [Epoch 20] - train loss: 0.0064 - validation loss: 0.0064 - (0.85s/epoch)\n",
      "⏳ [Epoch 30] - train loss: 0.0061 - validation loss: 0.0061 - (0.79s/epoch)\n",
      "⏳ [Epoch 30] - train loss: 0.0061 - validation loss: 0.0062 - (0.81s/epoch)\n",
      " --- 💾 Saving best model at epoch: 30 --- \n",
      "⏳ [Epoch 30] - train loss: 0.0061 - validation loss: 0.0059 - (0.81s/epoch)\n",
      "⏳ [Epoch 40] - train loss: 0.0057 - validation loss: 0.0059 - (0.84s/epoch)\n",
      " --- 💾 Saving best model at epoch: 40 --- \n",
      "⏳ [Epoch 40] - train loss: 0.0058 - validation loss: 0.0060 - (0.84s/epoch)\n",
      "⏳ [Epoch 40] - train loss: 0.0058 - validation loss: 0.0057 - (0.84s/epoch)\n",
      "⏳ [Epoch 50] - train loss: 0.0057 - validation loss: 0.0058 - (0.82s/epoch)\n",
      "⏳ [Epoch 50] - train loss: 0.0056 - validation loss: 0.0059 - (0.82s/epoch)\n",
      " --- 💾 Saving best model at epoch: 50 --- \n",
      "⏳ [Epoch 50] - train loss: 0.0056 - validation loss: 0.0056 - (0.82s/epoch)\n",
      "⏳ [Epoch 60] - train loss: 0.0056 - validation loss: 0.0057 - (0.83s/epoch)\n",
      "⏳ [Epoch 60] - train loss: 0.0054 - validation loss: 0.0058 - (0.83s/epoch)\n",
      " --- 💾 Saving best model at epoch: 60 --- \n",
      "⏳ [Epoch 60] - train loss: 0.0056 - validation loss: 0.0056 - (0.83s/epoch)\n",
      "⏳ [Epoch 70] - train loss: 0.0053 - validation loss: 0.0057 - (0.81s/epoch)\n",
      "⏳ [Epoch 70] - train loss: 0.0055 - validation loss: 0.0058 - (0.81s/epoch)\n",
      "⏳ [Epoch 70] - train loss: 0.0055 - validation loss: 0.0056 - (0.81s/epoch)\n",
      "⏳ [Epoch 80] - train loss: 0.0052 - validation loss: 0.0056 - (0.80s/epoch)\n",
      "⏳ [Epoch 80] - train loss: 0.0053 - validation loss: 0.0058 - (0.80s/epoch)\n",
      " --- 💾 Saving best model at epoch: 80 --- \n",
      "⏳ [Epoch 80] - train loss: 0.0055 - validation loss: 0.0055 - (0.80s/epoch)\n",
      "⏳ [Epoch 90] - train loss: 0.0052 - validation loss: 0.0056 - (0.83s/epoch)\n",
      "⏳ [Epoch 90] - train loss: 0.0053 - validation loss: 0.0058 - (0.83s/epoch)\n",
      " --- 💾 Saving best model at epoch: 90 --- \n",
      "⏳ [Epoch 90] - train loss: 0.0052 - validation loss: 0.0055 - (0.83s/epoch)\n",
      "⏳ [Epoch 100] - train loss: 0.0052 - validation loss: 0.0058 - (0.82s/epoch)\n",
      "⏳ [Epoch 100] - train loss: 0.0053 - validation loss: 0.0060 - (0.82s/epoch)\n",
      "⏳ [Epoch 100] - train loss: 0.0052 - validation loss: 0.0057 - (0.82s/epoch)\n",
      "⏳ [Epoch 110] - train loss: 0.0052 - validation loss: 0.0056 - (0.80s/epoch)\n",
      "⏳ [Epoch 110] - train loss: 0.0051 - validation loss: 0.0058 - (0.80s/epoch)\n",
      "⏳ [Epoch 110] - train loss: 0.0052 - validation loss: 0.0055 - (0.80s/epoch)\n",
      "⏳ [Epoch 120] - train loss: 0.0050 - validation loss: 0.0057 - (0.82s/epoch)\n",
      "⏳ [Epoch 120] - train loss: 0.0050 - validation loss: 0.0059 - (0.82s/epoch)\n",
      "⏳ [Epoch 120] - train loss: 0.0049 - validation loss: 0.0056 - (0.82s/epoch)\n",
      "⏳ [Epoch 130] - train loss: 0.0050 - validation loss: 0.0059 - (0.82s/epoch)\n",
      "⏳ [Epoch 130] - train loss: 0.0049 - validation loss: 0.0060 - (0.82s/epoch)\n",
      "⏳ [Epoch 130] - train loss: 0.0049 - validation loss: 0.0058 - (0.82s/epoch)\n",
      "⏳ [Epoch 140] - train loss: 0.0049 - validation loss: 0.0061 - (0.80s/epoch)\n",
      "⏳ [Epoch 140] - train loss: 0.0048 - validation loss: 0.0063 - (0.80s/epoch)\n",
      "⏳ [Epoch 140] - train loss: 0.0049 - validation loss: 0.0060 - (0.80s/epoch)\n",
      "⏳ [Epoch 150] - train loss: 0.0046 - validation loss: 0.0058 - (0.80s/epoch)\n",
      "⏳ [Epoch 150] - train loss: 0.0047 - validation loss: 0.0060 - (0.80s/epoch)\n",
      "⏳ [Epoch 150] - train loss: 0.0047 - validation loss: 0.0057 - (0.80s/epoch)\n",
      "⏳ [Epoch 160] - train loss: 0.0047 - validation loss: 0.0059 - (0.84s/epoch)\n",
      "⏳ [Epoch 160] - train loss: 0.0047 - validation loss: 0.0061 - (0.84s/epoch)\n",
      "⏳ [Epoch 160] - train loss: 0.0047 - validation loss: 0.0058 - (0.84s/epoch)\n",
      "⏳ [Epoch 170] - train loss: 0.0044 - validation loss: 0.0060 - (0.82s/epoch)\n",
      "⏳ [Epoch 170] - train loss: 0.0044 - validation loss: 0.0062 - (0.82s/epoch)\n",
      "⏳ [Epoch 170] - train loss: 0.0043 - validation loss: 0.0059 - (0.82s/epoch)\n",
      "⏳ [Epoch 180] - train loss: 0.0044 - validation loss: 0.0062 - (0.81s/epoch)\n",
      "⏳ [Epoch 180] - train loss: 0.0044 - validation loss: 0.0060 - (0.81s/epoch)\n",
      "⏳ [Epoch 180] - train loss: 0.0044 - validation loss: 0.0060 - (0.81s/epoch)\n",
      "Early stopping at epoch 186\n",
      "\n",
      " Training completed. Best validation loss: 0.0060\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import torch.distributed as dist\n",
    "\n",
    "# Clean up any existing process groups\n",
    "if dist.is_initialized():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "# Set environment explicitly\n",
    "env = os.environ.copy()\n",
    "env[\"PYTHONPATH\"] = f\"{root_dir}:{env.get('PYTHONPATH', '')}\"\n",
    "\n",
    "cmd = [\n",
    "    \"torchrun\",\n",
    "    \"--nproc_per_node=3\",  # Adjust based on available GPUs\n",
    "    \"--nnodes=1\",\n",
    "    f\"{root_dir}/PyISV/scripts/train_autoencoder.py\",\n",
    "    \"--config\", f\"{model_id_dir}/config.json\",\n",
    "    \"--models_dir\", models_dir,\n",
    "    \"--run_id\", run_id,\n",
    "]\n",
    "\n",
    "print(f\"Running command: {' '.join(cmd)}\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(cmd, check=True, text=True, capture_output=False, env=env)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Training failed with error code:\", e.returncode)\n",
    "    print(\"Output:\\n\", e.output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
