{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0d712c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and setup paths\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "\n",
    "from PyISV.utils.IO_utils import find_project_root\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Paths to the data\n",
    "root_dir = find_project_root()\n",
    "data_dir = os.path.join(root_dir, \"datasets\")\n",
    "models_dir = os.path.join(root_dir, \"models/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a47ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature map length: 21\n",
      "Calculated flattened dimension: 1344\n"
     ]
    }
   ],
   "source": [
    "#Configure training parameters\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "# Get input and target data paths\n",
    "input_data = f\"{data_dir}/RDFs/nonMin_nPt_85.pt\"\n",
    "target_data = f\"{data_dir}/RDFs/min_nPt_85.pt\"\n",
    "\n",
    "# Enable DDP\n",
    "use_ddp = True\n",
    "use_data_parallel = False\n",
    "\n",
    "# Model architecture parameters\n",
    "embed_dim = 3\n",
    "in_channels = 1\n",
    "input_length = 340\n",
    "n_features = 340 * in_channels\n",
    "\n",
    "# Encoder architecture\n",
    "channels = [16, 32, 64, 64]  # out_channels for each Conv1d\n",
    "kernel_sizes = [20, 15, 10, 5]\n",
    "paddings = [\"same\", \"same\", \"same\", \"same\"]  # Padding for each Conv1d\n",
    "strides = [1, 1, 1, 1]\n",
    "pool_kernel = 2\n",
    "pool_stride = 2\n",
    "\n",
    "length = input_length\n",
    "for i in range(len(channels)):\n",
    "    if paddings[i] == \"same\":\n",
    "        conv_length = math.ceil(length / strides[i])\n",
    "    else:\n",
    "        pad = paddings[i] if isinstance(paddings[i], int) else 0\n",
    "        conv_length = math.floor((length + 2*int(pad) - (kernel_sizes[i]-1) - 1)/strides[i] + 1)\n",
    "    length = math.floor((conv_length - pool_kernel)/pool_stride + 1) # Apply pooling\n",
    "\n",
    "\n",
    "last_layer_length = channels[-1]\n",
    "flat_dim = last_layer_length * length\n",
    "feature_map_length = length\n",
    "print(f\"Final feature map length: {length}\")\n",
    "print(f\"Calculated flattened dimension: {flat_dim}\")\n",
    "\n",
    "params = {\n",
    "  \"GENERAL\": {\n",
    "    \"device\": \"cuda\",\n",
    "    \"seed\": random.randint(0, 100000),\n",
    "    \"apply_jit_tracing\": False,\n",
    "    \"use_data_parallel\": use_data_parallel,\n",
    "    \"use_ddp\": use_ddp,\n",
    "    \"use_lr_finder\": False,\n",
    "    \"use_tensorboard\": False,\n",
    "    \"input_length\": 340,\n",
    "    \"input_channels\": in_channels,\n",
    "    \"input_features\": n_features,\n",
    "    \"flattened_features\": n_features\n",
    "  },\n",
    "  \"MODEL\": {\n",
    "    \"type\": \"autoencoder\",\n",
    "    \"input_shape\": [in_channels, n_features],\n",
    "    \"embedding_dim\": embed_dim,\n",
    "    \"flattened_dim\": flat_dim,\n",
    "    \"feature_map_length\": feature_map_length,\n",
    "    \"encoder_layers\": [\n",
    "      [\n",
    "        {\"type\": \"Conv1d\", \"in_channels\": in_channels, \"out_channels\": 16, \"kernel_size\": 20, \"stride\":1, \"padding\": \"same\"},\n",
    "        {\"type\": \"BatchNorm1d\", \"num_features\": 16},\n",
    "        {\"type\": \"Dropout\", \"p\": 0.2},\n",
    "        {\"type\": \"ReLU\"},\n",
    "        {\"type\": \"MaxPool1d\", \"kernel_size\": 2, \"stride\": 2}\n",
    "      ],\n",
    "      [\n",
    "        {\"type\": \"Conv1d\", \"in_channels\": 16, \"out_channels\": 32, \"kernel_size\": 15, \"stride\":1, \"padding\": \"same\"},\n",
    "        {\"type\": \"BatchNorm1d\", \"num_features\": 32},\n",
    "        {\"type\": \"Dropout\", \"p\": 0.15},\n",
    "        {\"type\": \"ReLU\"},\n",
    "        {\"type\": \"MaxPool1d\", \"kernel_size\": 2, \"stride\": 2}\n",
    "      ],\n",
    "      [\n",
    "        {\"type\": \"Conv1d\", \"in_channels\": 32, \"out_channels\": 64, \"kernel_size\": 10, \"stride\":1, \"padding\": \"same\"},\n",
    "        {\"type\": \"BatchNorm1d\", \"num_features\": 64},\n",
    "        {\"type\": \"ReLU\"},\n",
    "        {\"type\": \"MaxPool1d\", \"kernel_size\": 2, \"stride\": 2}\n",
    "      ],\n",
    "      [\n",
    "        {\"type\": \"Conv1d\", \"in_channels\": 64, \"out_channels\": last_layer_length, \"kernel_size\": 5, \"stride\":1, \"padding\": \"same\"},\n",
    "        {\"type\": \"BatchNorm1d\", \"num_features\": last_layer_length},\n",
    "        {\"type\": \"ReLU\"},\n",
    "        {\"type\": \"MaxPool1d\", \"kernel_size\": 2, \"stride\": 2}\n",
    "      ]\n",
    "    ],\n",
    "    \"bottleneck_layers\": [\n",
    "      [\n",
    "        {\"type\": \"Flatten\"},\n",
    "        {\"type\": \"Linear\", \"in_features\": flat_dim, \"out_features\": embed_dim},  # Fully connected layer\n",
    "      ],\n",
    "      [\n",
    "        {\"type\": \"Linear\", \"in_features\": embed_dim, \"out_features\": flat_dim}, # Embedding layer\n",
    "        {\"type\": \"ReLU\"},\n",
    "      ]\n",
    "    ],\n",
    "    \"decoder_layers\": [\n",
    "      [\n",
    "        {\"type\": \"Upsample\", \"scale_factor\": 2},\n",
    "        {\"type\": \"ConvTranspose1d\", \"in_channels\": last_layer_length, \"out_channels\": 64, \"kernel_size\": 5, \"padding\": 1},\n",
    "        {\"type\": \"ReLU\"},\n",
    "        {\"type\": \"BatchNorm1d\", \"num_features\": 64},\n",
    "      ],\n",
    "      [\n",
    "        {\"type\": \"Upsample\", \"scale_factor\": 2},\n",
    "        {\"type\": \"ConvTranspose1d\", \"in_channels\": 64, \"out_channels\": 32, \"kernel_size\": 10, \"padding\": 1},\n",
    "        {\"type\": \"ReLU\"},\n",
    "        {\"type\": \"BatchNorm1d\", \"num_features\": 32},\n",
    "      ],\n",
    "      [\n",
    "        {\"type\": \"Upsample\", \"scale_factor\": 2},\n",
    "        {\"type\": \"ConvTranspose1d\", \"in_channels\": 32, \"out_channels\": 16, \"kernel_size\": 15, \"padding\": 1},\n",
    "        {\"type\": \"ReLU\"},\n",
    "        {\"type\": \"BatchNorm1d\", \"num_features\": 16},\n",
    "      ],\n",
    "      [\n",
    "        {\"type\": \"Upsample\", \"scale_factor\": 2},\n",
    "        {\"type\": \"Conv1d\", \"in_channels\": 16, \"out_channels\": in_channels, \"kernel_size\": 20, \"padding\": 1},\n",
    "      ]\n",
    "    ]\n",
    "  },\n",
    "  \"TRAINING\": {\n",
    "    # Training parameters\n",
    "    \"batch_size\": 1024,\n",
    "    \"train_size\": 0.8,\n",
    "    \"min_epochs\": 150,\n",
    "    \"max_epochs\": 500,\n",
    "    \"loss_function\": \"HuberLoss\",\n",
    "    \"loss_params\": {\n",
    "      \"reduction\": \"mean\",\n",
    "      \"delta\": 1.0\n",
    "      },\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"normalization\": \"minmax\",\n",
    "    \"weight_decay\": 0.0005,\n",
    "    \n",
    "    # Optimizer parameters\n",
    "    \"num_workers\": 0,\n",
    "    \"pin_memory\": False,\n",
    "    \"scheduled_lr\": True,\n",
    "    \"scheduler_params\": {\n",
    "      \"lr_warmup_epochs\": 10,\n",
    "      \"milestones\": [100, 150, 200],\n",
    "        \"gamma\": 0.5\n",
    "    },\n",
    "    \"early_stopping\": True,\n",
    "    \"early_stopping_params\": {\n",
    "      \"patience\": 30,\n",
    "      \"min_delta\": 0.00005\n",
    "    },\n",
    "  },\n",
    "  \"INPUTS\": {\n",
    "    \"dataset\": input_data,\n",
    "    \"target\": target_data\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b23f998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Configuration Summary ===\n",
      "Run ID: nonMin_to_min_nPt_85\n",
      "DDP Enabled: True\n",
      "Batch Size: 1024\n",
      "Number of Workers: 0\n",
      "Learning Rate: 0.001\n",
      "\n",
      "=== Available CPU Resources ===\n",
      "CPUs available to PyTorch: 64\n",
      "Num of OpenMP threads: 16\n",
      "\n",
      "=== Available GPU Resources ===\n",
      "GPUs available to PyTorch: 2\n",
      "Available GPU devices: ['NVIDIA GeForce RTX 2080 Ti', 'NVIDIA GeForce RTX 2080 Ti']\n"
     ]
    }
   ],
   "source": [
    "# Save json configuration\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "#run_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_id = \"nonMin_to_min_nPt_85\"\n",
    "model_id_dir = f\"{models_dir}/{run_id}\"\n",
    "os.makedirs(model_id_dir, exist_ok=True)\n",
    "\n",
    "with open(f\"{model_id_dir}/config.json\", 'w') as f:\n",
    "    json.dump(params, f, indent=4)\n",
    "\n",
    "# Set up environment variables\n",
    "master_port = random.randint(29500, 30000)  # Random port for DDP\n",
    "os.environ.update({\n",
    "    #\"NCCL_DEBUG\": \"INFO\",                          # Enable NCCL debugging\n",
    "    \"NCCL_SOCKET_IFNAME\": \"^lo,docker\",             # Skip loopback and docker interfaces\n",
    "    \"NCCL_IB_DISABLE\": \"0\",                         # Enable InfiniBand if available\n",
    "    \"NCCL_P2P_DISABLE\": \"0\",                        # Ensure P2P is enabled\n",
    "    \"TORCH_NCCL_BLOCKING_WAIT\": \"1\",                # Use blocking wait for better performance\n",
    "    \"NCCL_LL_THRESHOLD\": \"0\",                       # Disable low latency threshold \n",
    "    \"MASTER_PORT\": str(master_port),                # Random port for DDP\n",
    "    \"MASTER_ADDR\": \"localhost\",                     # Master address for DDP\n",
    "    \"WORLD_SIZE\": str(torch.cuda.device_count()),   # Total number of possible processes\n",
    "    \"OMP_NUM_THREADS\": \"16\",                        # Set OpenMP threads to 16\n",
    "    \"MKL_THREADING_LAYER\": \"INTEL\",                 # Set MKL threading layer to Intel\n",
    "    \"KMP_BLOCKTIME\": \"0\",                           # Set KMP block time to 0\n",
    "    \"KMP_AFFINITY\": \"granularity=fine,compact,1,0\", # Set KMP affinity\n",
    "    \"KMP_HW_SUBSET\": \"1t\",                          # Use only physical cores, no hyperthreading\n",
    "    \"I_MPI_PIN_DOMAIN\": \"auto\",                     # Automatically pin MPI processes to cores\n",
    "    \"I_MPI_PIN\": \"ON\",                              # Enable process pinning\n",
    "    \"I_MPI_PIN_CELL\": \"core\",                       # Pin MPI processes to cores\n",
    "    \"CUDA_VISIBLE_DEVICES\": \",\".join(str(i) for i in range(torch.cuda.device_count())),\n",
    "    \"PYTHONPATH\": f\"{root_dir}:{os.environ.get('PYTHONPATH', '')}\"\n",
    "})\n",
    "\n",
    "# Print configuration summary\n",
    "print(\"\\n=== Configuration Summary ===\")\n",
    "print(f\"Run ID: {run_id}\")\n",
    "print(f\"DDP Enabled: {use_ddp}\")\n",
    "print(f\"Batch Size: {params['TRAINING']['batch_size']}\")\n",
    "print(f\"Number of Workers: {params['TRAINING']['num_workers']}\")\n",
    "print(f\"Learning Rate: {params['TRAINING']['learning_rate']}\")\n",
    "\n",
    "# Check for generic variables that might be available\n",
    "torch.set_num_threads(int(os.environ.get('OMP_NUM_THREADS', 1)))\n",
    "print(\"\\n=== Available CPU Resources ===\")\n",
    "print(f\"CPUs available to PyTorch: {torch.get_num_interop_threads()}\")\n",
    "print(f\"Num of OpenMP threads: {torch.get_num_threads()}\")\n",
    "\n",
    "print(\"\\n=== Available GPU Resources ===\")\n",
    "print(f\"GPUs available to PyTorch: {torch.cuda.device_count()}\")\n",
    "print(f\"Available GPU devices: {[torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1ed6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: torchrun --nproc_per_node=2 --nnodes=1 /home/shared_folder/PyISV/PyISV/scripts/train_CNN.py --config /home/shared_folder/PyISV/models//nonMin_to_min_nPt_85/config.json --models_dir /home/shared_folder/PyISV/models/ --run_id nonMin_to_min_nPt_85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DDP] Detected torchrun: LOCAL_RANK=1, RANK=1, WORLD_SIZE=2\n",
      "[DDP] Detected torchrun: LOCAL_RANK=0, RANK=0, WORLD_SIZE=2\n",
      "ℹ️ Using GPU backend: nccl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank1]:[W527 13:05:03.425448743 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n",
      "[rank0]:[W527 13:05:03.461684163 ProcessGroupNCCL.cpp:4715] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can pecify device_id in init_process_group() to force use of a particular device.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DDP initialized successfully with 2 processes using nccl backend\n",
      "\n",
      "Training on cuda with run ID: nonMin_to_min_nPt_85\n",
      "\n",
      "ℹ️ Model type: <class 'torch.nn.parallel.distributed.DistributedDataParallel'>\n",
      "ℹ️ Device: cuda:0\n",
      "ℹ️ Use DDP: True\n",
      "ℹ️ Loss function: HuberLoss()\n",
      "\n",
      "▶️ Starting training from epoch 0 to 500\n",
      "\n",
      " --- 💾 Saving best model at epoch: 10 --- \n",
      "⏳ [Epoch 10]  - train loss: 0.0022  - validation loss: 0.0034  - lr: 0.00091  - (1.39s/epoch)\n",
      "⏳ [Epoch 10]  - train loss: 0.0023  - validation loss: 0.0034  - lr: 0.00091  - (1.40s/epoch)\n",
      "⏳ [Epoch 20]  - train loss: 0.0020  - validation loss: 0.0045  - lr: 0.00100  - (1.30s/epoch)\n",
      "⏳ [Epoch 20]  - train loss: 0.0020  - validation loss: 0.0045  - lr: 0.00100  - (1.30s/epoch)\n",
      "⏳ [Epoch 30]  - train loss: 0.0019  - validation loss: 0.0042  - lr: 0.00100  - (1.28s/epoch)\n",
      "⏳ [Epoch 30]  - train loss: 0.0020  - validation loss: 0.0042  - lr: 0.00100  - (1.28s/epoch)\n",
      " --- 💾 Saving best model at epoch: 40 --- \n",
      "⏳ [Epoch 40]  - train loss: 0.0020  - validation loss: 0.0030  - lr: 0.00100  - (1.28s/epoch)\n",
      "⏳ [Epoch 40]  - train loss: 0.0020  - validation loss: 0.0030  - lr: 0.00100  - (1.29s/epoch)\n",
      "⏳ [Epoch 50]  - train loss: 0.0020  - validation loss: 0.0028  - lr: 0.00100  - (1.29s/epoch)\n",
      " --- 💾 Saving best model at epoch: 50 --- \n",
      "⏳ [Epoch 50]  - train loss: 0.0020  - validation loss: 0.0028  - lr: 0.00100  - (1.31s/epoch)\n",
      " --- 💾 Saving best model at epoch: 60 --- \n",
      "⏳ [Epoch 60]  - train loss: 0.0020  - validation loss: 0.0025  - lr: 0.00100  - (1.29s/epoch)\n",
      "⏳ [Epoch 60]  - train loss: 0.0020  - validation loss: 0.0024  - lr: 0.00100  - (1.30s/epoch)\n",
      " --- 💾 Saving best model at epoch: 70 --- \n",
      "⏳ [Epoch 70]  - train loss: 0.0020  - validation loss: 0.0022  - lr: 0.00100  - (1.31s/epoch)\n",
      "⏳ [Epoch 70]  - train loss: 0.0020  - validation loss: 0.0022  - lr: 0.00100  - (1.31s/epoch)\n",
      " --- 💾 Saving best model at epoch: 80 --- \n",
      "⏳ [Epoch 80]  - train loss: 0.0019  - validation loss: 0.0022  - lr: 0.00100  - (1.30s/epoch)\n",
      "⏳ [Epoch 80]  - train loss: 0.0019  - validation loss: 0.0022  - lr: 0.00100  - (1.30s/epoch)\n",
      "⏳ [Epoch 90]  - train loss: 0.0019  - validation loss: 0.0022  - lr: 0.00100  - (1.31s/epoch)\n",
      "⏳ [Epoch 90]  - train loss: 0.0020  - validation loss: 0.0022  - lr: 0.00100  - (1.32s/epoch)\n",
      " --- 💾 Saving best model at epoch: 100 --- \n",
      "⏳ [Epoch 100]  - train loss: 0.0019  - validation loss: 0.0022  - lr: 0.00100  - (1.33s/epoch)\n",
      "⏳ [Epoch 100]  - train loss: 0.0019  - validation loss: 0.0022  - lr: 0.00100  - (1.32s/epoch)\n",
      " --- 💾 Saving best model at epoch: 110 --- \n",
      "⏳ [Epoch 110]  - train loss: 0.0019  - validation loss: 0.0021  - lr: 0.00100  - (1.30s/epoch)\n",
      "⏳ [Epoch 110]  - train loss: 0.0019  - validation loss: 0.0021  - lr: 0.00100  - (1.30s/epoch)\n",
      "⏳ [Epoch 120]  - train loss: 0.0019  - validation loss: 0.0023  - lr: 0.00050  - (1.32s/epoch)\n",
      "⏳ [Epoch 120]  - train loss: 0.0019  - validation loss: 0.0023  - lr: 0.00050  - (1.33s/epoch)\n",
      " --- 💾 Saving best model at epoch: 130 --- \n",
      "⏳ [Epoch 130]  - train loss: 0.0019  - validation loss: 0.0021  - lr: 0.00050  - (1.32s/epoch)\n",
      "⏳ [Epoch 130]  - train loss: 0.0019  - validation loss: 0.0021  - lr: 0.00050  - (1.31s/epoch)\n",
      "⏳ [Epoch 140]  - train loss: 0.0019  - validation loss: 0.0021  - lr: 0.00050  - (1.30s/epoch)\n",
      "⏳ [Epoch 140]  - train loss: 0.0019  - validation loss: 0.0021  - lr: 0.00050  - (1.30s/epoch)\n",
      "⏳ [Epoch 150]  - train loss: 0.0019  - validation loss: 0.0021  - lr: 0.00050  - (1.31s/epoch)\n",
      "⏳ [Epoch 150]  - train loss: 0.0019  - validation loss: 0.0021  - lr: 0.00050  - (1.32s/epoch)\n",
      "⏳ [Epoch 160]  - train loss: 0.0019  - validation loss: 0.0021  - lr: 0.00050  - (1.30s/epoch)\n",
      "⏳ [Epoch 160]  - train loss: 0.0019  - validation loss: 0.0021  - lr: 0.00050  - (1.31s/epoch)\n",
      " --- 💾 Saving best model at epoch: 170 --- \n",
      "⏳ [Epoch 170]  - train loss: 0.0019  - validation loss: 0.0020  - lr: 0.00025  - (1.31s/epoch)\n",
      "⏳ [Epoch 170]  - train loss: 0.0019  - validation loss: 0.0020  - lr: 0.00025  - (1.32s/epoch)\n",
      "⏳ [Epoch 180]  - train loss: 0.0019  - validation loss: 0.0021  - lr: 0.00025  - (1.34s/epoch)\n",
      "⏳ [Epoch 180]  - train loss: 0.0019  - validation loss: 0.0021  - lr: 0.00025  - (1.33s/epoch)\n",
      "Early stopping at epoch 184\n",
      "\n",
      "Training completed. Best validation loss: 0.0020\n",
      "Training statistics saved to /home/shared_folder/PyISV/models//nonMin_to_min_nPt_85/stats/stats.npz\n"
     ]
    }
   ],
   "source": [
    "import torch.multiprocessing as mp\n",
    "import subprocess\n",
    "import torch.distributed as dist\n",
    "\n",
    "# Clean up any existing process groups\n",
    "torch.cuda.empty_cache()\n",
    "if dist.is_initialized():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "cmd = [\n",
    "    \"torchrun\",\n",
    "    \"--nproc_per_node=2\",  # Adjust based on available GPUs\n",
    "    \"--nnodes=1\",\n",
    "    f\"{root_dir}/PyISV/scripts/train_CNN.py\",\n",
    "    \"--config\", f\"{model_id_dir}/config.json\",\n",
    "    \"--models_dir\", models_dir,\n",
    "    \"--run_id\", run_id,\n",
    "]\n",
    "\n",
    "print(f\"Running command: {' '.join(cmd)}\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(cmd, check=True, text=True, capture_output=False)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Training failed with error code:\", e.returncode)\n",
    "    print(\"Output:\\n\", e.output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
